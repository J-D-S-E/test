{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJXvUy6hqugD8goIisuOox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-D-S-E/test/blob/main/Agente_de_IA_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipCQJJszQcfY",
        "outputId": "fbd1ba81-3fca-44f6-e9b8-039eaf31b1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # Biblioteca para fazer requisições HTTP\n",
        "import json      # Biblioteca para manipular dados JSON\n",
        "\n",
        "# Substitua com sua chave de API do Gemini\n",
        "API_KEY = 'TOKEN'\n",
        "# URL da API do modelo Gemini 1.5 para geração de conteúdo\n",
        "GEMINI_API_URL = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}'\n",
        "\n",
        "def make_gemini_request(prompt, functions=None):\n",
        "    \"\"\"\n",
        "    Função para fazer uma requisição ao modelo Gemini 1.5, com suporte opcional a chamadas de função.\n",
        "\n",
        "    Parâmetros:\n",
        "    - prompt (str): O texto de entrada para o modelo.\n",
        "    - functions (list, opcional): Lista de funções que o modelo pode chamar automaticamente.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dados básicos para a requisição, incluindo o conteúdo do prompt e configurações de geração\n",
        "    data = {\n",
        "        'contents': [\n",
        "            {\n",
        "                'role': 'user',  # Define o papel do conteúdo como input do usuário\n",
        "                'parts': [\n",
        "                    {\n",
        "                        'text': prompt  # Insere o prompt do usuário\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        'generationConfig': {  # Configurações de geração de texto\n",
        "            'temperature': 1,         # Controla a criatividade da resposta\n",
        "            'topK': 64,               # Limita os tokens mais prováveis a considerar\n",
        "            'topP': 0.95,             # Probabilidade cumulativa para limitar tokens\n",
        "            'maxOutputTokens': 1024,  # Máximo de tokens para a resposta\n",
        "            'responseMimeType': 'text/plain'  # Tipo MIME da resposta esperada\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Se funções forem passadas, adiciona à configuração da requisição\n",
        "    if functions:\n",
        "        # Adiciona as funções definidas para o modelo\n",
        "        data['tools'] = [{\n",
        "            'function_declarations': functions\n",
        "        }]\n",
        "\n",
        "        # Configuração de chamadas de função, controlando como o modelo usa as funções\n",
        "        data['toolConfig'] = {\n",
        "            'functionCallingConfig': {\n",
        "                'mode': 'AUTO'  # Modo de chamada de função: 'AUTO', 'NONE', ou 'ANY'\n",
        "            }\n",
        "        }\n",
        "        # Explicação dos modos:\n",
        "        # 'AUTO' permite que o modelo decida entre resposta em texto ou chamada de função\n",
        "        # 'NONE' força o modelo a responder apenas em texto\n",
        "        # 'ANY' força o uso de uma chamada de função (disponível apenas no Gemini 1.5 Pro)\n",
        "        \"\"\"# Forçando uso de função específica\n",
        "        data['toolConfig'] = {\n",
        "            'functionCallingConfig': {\n",
        "                'mode': 'ANY',\n",
        "                'allowedFunctionNames': ['get_weather']  # Só permite chamar esta função\n",
        "            }\n",
        "        }\"\"\"\n",
        "\n",
        "    # Cabeçalho para a requisição, especificando que o conteúdo é JSON\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    # Envia a requisição POST para a API do Gemini com os dados configurados\n",
        "    response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(data))\n",
        "\n",
        "    # Se a resposta for bem-sucedida (status code 200), extrai o conteúdo JSON\n",
        "    if response.status_code == 200:\n",
        "        response_json = response.json()\n",
        "        # Verifica se existem 'candidates' na resposta (possíveis respostas do modelo)\n",
        "        if 'candidates' in response_json and len(response_json['candidates']) > 0:\n",
        "            return response_json  # Retorna a resposta completa como JSON\n",
        "    return None  # Retorna None se a requisição falhar ou não houver candidatos\n",
        "\n",
        "# Exemplo de declarações de funções que o modelo pode chamar\n",
        "example_functions = [\n",
        "    {\n",
        "        \"name\": \"get_weather\",  # Nome da função\n",
        "        \"description\": \"Get the current weather in a given location\",  # Descrição da função\n",
        "        \"parameters\": {  # Parâmetros esperados pela função\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state\"  # Descrição do parâmetro 'location'\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"]  # Define 'location' como obrigatório\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_stock_price\",  # Nome da função\n",
        "        \"description\": \"Get the current stock price for a given symbol\",  # Descrição da função\n",
        "        \"parameters\": {  # Parâmetros esperados pela função\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"symbol\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The stock symbol\"  # Descrição do parâmetro 'symbol'\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"symbol\"]  # Define 'symbol' como obrigatório\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Exemplo de uso da função `make_gemini_request` com um prompt e funções\n",
        "prompt = \"Qual a Previsão do Tempo em Los Angeles?\"  # Exemplo de prompt perguntando sobre a cotação da Apple\n",
        "response = make_gemini_request(prompt, example_functions)\n",
        "if response:\n",
        "    print('Response:', response)  # Exibe a resposta, se houver\n",
        "else:\n",
        "    print('Failed to get response')  # Mensagem de erro caso a resposta falhe\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_n-CW9uQol0",
        "outputId": "0058c7bc-021a-4c6c-cffe-df12b74f8878"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: {'candidates': [{'content': {'parts': [{'functionCall': {'name': 'get_weather', 'args': {'location': 'Los Angeles'}}}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0, 'safetyRatings': [{'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}], 'usageMetadata': {'promptTokenCount': 99, 'candidatesTokenCount': 16, 'totalTokenCount': 115}, 'modelVersion': 'gemini-1.5-flash-001'}\n"
          ]
        }
      ]
    }
  ]
}
